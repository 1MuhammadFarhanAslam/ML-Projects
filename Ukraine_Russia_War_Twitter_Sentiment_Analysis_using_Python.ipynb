{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPk/z3xRdf66taEIJZw8JDh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1MuhammadFarhanAslam/ML-Projects/blob/main/Ukraine_Russia_War_Twitter_Sentiment_Analysis_using_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "**Many countries including west are supporting Ukraine by introducing economic sanctions on Russia. There are a lot of tweets about the Ukraine and Russia war where people tend to update about the ground truths, what they feel about it, and who they are supporting. So we will analyze the sentiments of people over the Ukraine and Russian War.**"
      ],
      "metadata": {
        "id": "Qz5hbGzO5lxF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Mounting Google Drive**"
      ],
      "metadata": {
        "id": "r11HgIAb7R_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "G75C8jXg7Nl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Configure Google Colab to Kaggle through Kaggle API**\n",
        "\n",
        "**To connect Kaggle datasets to Google Colab, you need to follow these steps:**\n",
        "\n",
        "* 1: Install the Kaggle library in Google Colab by running the following command"
      ],
      "metadata": {
        "id": "KHXTSzgm7ffb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "metadata": {
        "id": "eDaTA5Lv7twq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Go to the Kaggle website (https://www.kaggle.com) and sign in to your account (or create a new account if you don't have one).**\n",
        "\n",
        "*Navigate to the dataset you want to use in your Colab notebook.*\n",
        "\n",
        "*Click on the \"Copy API command\" button below the dataset description. This will copy the command to download the dataset using the Kaggle API.*\n",
        "\n",
        "*In your Colab notebook, import the necessary libraries and set up the Kaggle API by running the following code*"
      ],
      "metadata": {
        "id": "Ci0ebvQ673f2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# Upload your Kaggle API key file (kaggle.json) to Colab using the file upload feature\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "# Read the contents of the kaggle.json file\n",
        "with open('kaggle.json', 'r') as file:\n",
        "    kaggle_json = json.load(file)"
      ],
      "metadata": {
        "id": "bETspSxs7yKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Important about Kaggle API Security**\n",
        "\n",
        "**The command !chmod 600 ~/.kaggle/kaggle.json is used to change the permissions of the kaggle.json file to restrict access permissions.**\n",
        "\n",
        "*In Linux-based systems, including Google Colab, file permissions are represented by a three-digit number: the first digit represents the owner's permissions, the second digit represents the group's permissions, and the third digit represents other users' permissions.*\n",
        "\n",
        "**Here's a breakdown of what chmod 600 does:**\n",
        "\n",
        "* ***6 means the owner (the user who uploaded the kaggle.json file) has read and write permissions (4 for read and 2 for write), but no execute permissions (0 for execute). 0 means the group and other users have no permissions to read, write, or execute the file.***\n",
        "\n",
        "* ***By setting the permissions to chmod 600, it ensures that only the owner of the file (the user who uploaded the kaggle.json file) has read and write access, and no other users (group or others) can access or modify the file.***\n",
        "\n",
        "* **This step is important to maintain the security of your Kaggle API key, as it contains sensitive information and should not be accessible to other users of the system.**"
      ],
      "metadata": {
        "id": "TO-_dh7V8LgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Move the saved kaggle.json file to the required directory\n",
        "os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "os.rename('kaggle.json', '/root/.kaggle/kaggle.json')\n",
        "\n",
        "# Set the appropriate permissions for the Kaggle API key file\n",
        "os.chmod('/root/.kaggle/kaggle.json', 0o600)"
      ],
      "metadata": {
        "id": "FhT7nHxt8K72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "or"
      ],
      "metadata": {
        "id": "SRgKgeso8aN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Specify the path to the kaggle.json file\n",
        "kaggle_json_path = os.path.join(os.path.expanduser(\"~\"), \".kaggle\", \"kaggle.json\")\n",
        "\n",
        "# Check if the kaggle.json file already exists\n",
        "if os.path.exists(kaggle_json_path):\n",
        "    print(\"kaggle.json file already exists.\")\n",
        "else:\n",
        "    # Move the uploaded Kaggle API key file to the required directory\n",
        "    !mkdir -p ~/.kaggle    # This command creates a directory named '.kaggle' inside the user's home directory (~). The -p option ensures that the parent directories are also created if they don't exist. If the directory already exists, this command will not throw an error\n",
        "    !mv kaggle.json ~/.kaggle/    # This command moves the file named 'kaggle.json' to the ~/.kaggle/ directory. The mv command is used for file or directory relocation. The first argument, kaggle.json, represents the current name/path of the file, and the second argument, ~/.kaggle/, represents the destination directory where the file should be moved.\n",
        "    !chmod 600 ~/.kaggle/kaggle.json\n",
        "    print(\"kaggle.json file moved and permissions set successfully.\")\n"
      ],
      "metadata": {
        "id": "G-XrH0Sw8cXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Verifying Kaggle API**"
      ],
      "metadata": {
        "id": "7OWsHAmD8fGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify the Kaggle API is working\n",
        "!kaggle datasets list"
      ],
      "metadata": {
        "id": "wh23JJYH8i6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Downloading dataset from kaggle**"
      ],
      "metadata": {
        "id": "5Wowj1bvfiqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download --force towhidultonmoy/russia-vs-ukraine-tweets-datasetdaily-updated"
      ],
      "metadata": {
        "id": "puZ8SgeM8myC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**If the Kaggle API is working correctly, you can download the dataset by running the copied API command in your Colab notebook:**\n",
        "\n",
        "* **The -d flag is useful if you want to download the dataset only once. If you use the -d flag and the dataset already exists in your local directory, Kaggle will not download the dataset again.In your case, the dataset is being updated daily, so you may want to use the --force flag to make sure that you always have the latest version of the dataset.**"
      ],
      "metadata": {
        "id": "vbDVHR628mKV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The dataset will be downloaded as a ZIP file. You can unzip the file using the following command**"
      ],
      "metadata": {
        "id": "St65cgWq8qrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Specify the path to the ZIP file\n",
        "zip_file_path = '/content/russia-vs-ukraine-tweets-datasetdaily-updated.zip'\n",
        "\n",
        "# creating directory to unzip dataset\n",
        "!mkdir -p /content/russia-vs-ukraine-tweets-datasetdaily-updated\n",
        "\n",
        "# Specify the target directory to extract the files\n",
        "target_directory = '/content/russia-vs-ukraine-tweets-datasetdaily-updated'\n",
        "\n",
        "# Open the ZIP file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    # Extract all the files to the target directory\n",
        "    zip_ref.extractall(target_directory)\n",
        "\n",
        "print(\"ZIP file extracted successfully.\")"
      ],
      "metadata": {
        "id": "HFNVVLnn8rQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Specify the directory path\n",
        "directory_path = '/content/russia-vs-ukraine-tweets-datasetdaily-updated'\n",
        "\n",
        "# Create the directory if it doesn't already exist\n",
        "if not os.path.exists(directory_path):\n",
        "    os.makedirs(directory_path)\n",
        "    print(f\"Directory '{directory_path}' created successfully.\")\n",
        "else:\n",
        "    print(f\"Directory '{directory_path}' already exists.\")\n"
      ],
      "metadata": {
        "id": "b3ZtxFAi8yJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overview of Libraries using for Sentiment Analysis:\n",
        "\n",
        "1. **pandas**: pandas is a powerful data manipulation and analysis library in Python. It is commonly used for tasks like data cleaning, transformation, and exploration.\n",
        "\n",
        "2. **seaborn**: seaborn is a data visualization library built on top of matplotlib. It provides a high-level interface for creating informative and attractive statistical graphics. Seaborn simplifies the process of creating visualizations such as scatter plots, line plots, bar plots, histograms, and more.\n",
        "\n",
        "3. **matplotlib**: matplotlib is a widely used plotting library in Python. Matplotlib can be used to generate line plots, scatter plots, bar plots, histograms, pie charts, and many other types of visualizations.\n",
        "\n",
        "4. **nltk.sentiment.vader**: NLTK (Natural Language Toolkit) is a popular library for natural language processing in Python. It provides various tools and resources for tasks like tokenization, stemming, tagging, parsing, and sentiment analysis. The `vader` module within NLTK implements the VADER (Valence Aware Dictionary and sEntiment Reasoner) sentiment analysis algorithm, which is specifically designed for analyzing sentiment in social media texts.\n",
        "\n",
        "5. **wordcloud**: wordcloud is a library used for generating word clouds in Python. A word cloud is a visual representation of text data, where the size of each word corresponds to its frequency or importance. The `WordCloud` class in the wordcloud library allows you to create and customize word clouds based on your text data.\n",
        "\n",
        "6. **nltk**: It is a comprehensive library for working with human language data and performing various natural language processing tasks. It provides a wide range of functionalities, including tokenization, stemming, tagging, parsing, and more.\n",
        "\n",
        "7. **re**: re is the built-in regular expression module in Python. It provides functions and methods for working with regular expressions.The `re` module is often used for tasks like searching, extracting, and replacing specific patterns of text in strings.\n",
        "\n",
        "8. **nltk.corpus.stopwords**: nltk.corpus.stopwords is a collection of commonly used stopwords (i.e., words that are considered irrelevant for text analysis) in different languages. The stopwords module from the NLTK corpus contains pre-defined lists of stopwords that can be used to filter out these words from your text data.\n",
        "\n",
        "9. **string**: string is a built-in module in Python that provides various useful functions for working with strings. It includes a collection of ASCII characters, such as punctuation marks and whitespace, as well as functions for formatting, manipulating, and comparing strings."
      ],
      "metadata": {
        "id": "RXiZ39hl27Om"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing essential libraries**"
      ],
      "metadata": {
        "id": "yrOsr3VJ83TA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textblob"
      ],
      "metadata": {
        "id": "q4nuYc4pdGUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from textblob import TextBlob"
      ],
      "metadata": {
        "id": "PrMaSXfj4OUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reading CSV data**"
      ],
      "metadata": {
        "id": "kXSe1E_oCNkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/russia-vs-ukraine-tweets-datasetdaily-updated/filename.csv\")\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "59v8tnZh_ddj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "nNeA5-IfAy3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.describe())"
      ],
      "metadata": {
        "id": "NQ1BgZ-rA0Me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.info())"
      ],
      "metadata": {
        "id": "T7bmcHZBA68N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let’s have a quick look at all the column names of the dataset:**"
      ],
      "metadata": {
        "id": "-LdlteC6BSJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.columns)"
      ],
      "metadata": {
        "id": "ng4KieTuBJ4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We only need three columns for this task (username, tweet, and language); I will only select these columns and move forward:**"
      ],
      "metadata": {
        "id": "4D0_UccwB6p8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[['username', 'tweet','language']]\n",
        "data"
      ],
      "metadata": {
        "id": "bsEEntI_BVYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"tweet\"][0] # reading tweet having 0 index."
      ],
      "metadata": {
        "id": "Y7iaH51QYlgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let’s have a look at whether any of these columns contains any null values or not:**"
      ],
      "metadata": {
        "id": "S5a2QOYDCRd_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "PjENZgcoCKXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**So none of the columns has null values, let’s have a quick look at how many tweets are posted in which language:**"
      ],
      "metadata": {
        "id": "-mzzsl-_Cday"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['language'].value_counts()"
      ],
      "metadata": {
        "id": "ad49B33DCX3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.language.value_counts().sort_values().plot(kind = 'pie')"
      ],
      "metadata": {
        "id": "yGCt1cXkX2-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Count of unique languages present in the 'language' column of the DataFrame.**"
      ],
      "metadata": {
        "id": "GQdZKVAqEJ_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_languages = len(data['language'].value_counts())\n",
        "print(\"Total number of languages used:\", total_languages)"
      ],
      "metadata": {
        "id": "Z1IPRjM9Cl_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function to extract hashtags and generate barplot of the most frequent hashtags**"
      ],
      "metadata": {
        "id": "dKleLRZ1CcqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract hashtags from a list of tweets\n",
        "def hashtag_extract(text_list):\n",
        "    # Create a list to store the hashtags\n",
        "    hashtags = []\n",
        "\n",
        "    # Loop over the tweets\n",
        "    for text in text_list:\n",
        "        # Use the `re` module to find all of the hashtags in the tweet\n",
        "        ht = re.findall(r\"#(\\w+)\", text)\n",
        "\n",
        "        # Append the hashtags to the list\n",
        "        hashtags.append(ht)\n",
        "\n",
        "    # Return the list of hashtags\n",
        "    return hashtags\n",
        "\n",
        "# Function to generate a barplot of the most frequent hashtags\n",
        "def generate_hashtag_freqdist(hashtags):\n",
        "    # Create a frequency distribution of the hashtags\n",
        "    a = nltk.FreqDist(hashtags)\n",
        "\n",
        "    # Convert the frequency distribution to a Pandas DataFrame\n",
        "    d = pd.DataFrame({'Hashtag': list(a.keys()),\n",
        "                      'Count': list(a.values())})\n",
        "\n",
        "    # Select the top 25 most frequent hashtags\n",
        "    d = d.nlargest(columns=\"Count\", n = 25)\n",
        "\n",
        "    # Create a figure with the specified size\n",
        "    plt.figure(figsize=(16, 7))\n",
        "\n",
        "    # Create a barplot of the most frequent hashtags\n",
        "    ax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\n",
        "\n",
        "    # Rotate the x-ticks by 80 degrees\n",
        "    plt.xticks(rotation=80)\n",
        "\n",
        "    # Set the y-label\n",
        "    ax.set(ylabel = 'Count')\n",
        "\n",
        "    # Show the figure\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "-ru1_0HQY8vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* *The first function, hashtag_extract(), takes a list of tweets as input and returns a list of hashtags. The function works by looping over the tweets and using the re module to find all of the hashtags in each tweet. The hashtags are then returned as a list.*\n",
        "\n",
        "* *The second function, generate_hashtag_freqdist(), takes a list of hashtags as input and returns a barplot of the most frequent hashtags. The function works by first creating a frequency distribution of the hashtags using the nltk.FreqDist() function. The frequency distribution is then converted into a Pandas DataFrame and the top 25 hashtags are selected. The barplot is then created using the Seaborn library.*"
      ],
      "metadata": {
        "id": "P3CqzOntZCx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hashtags = hashtag_extract(data[\"tweet\"])\n",
        "hashtags = sum(hashtags, [])"
      ],
      "metadata": {
        "id": "Go2v_tUfaJuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_hashtag_freqdist(hashtags)"
      ],
      "metadata": {
        "id": "PVNeV8braO_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preparation of data**"
      ],
      "metadata": {
        "id": "EYVxf7ffC2nd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let’s prepare this data for the task of sentiment analysis. Here I will remove all the links, punctuation, symbols and other language errors from the tweets through:**\n",
        "\n",
        "*nltk.download('stopwords')*\n",
        "\n",
        "*stemmer = nltk.SnowballStemmer(\"english\")*\n",
        "\n",
        "*stopword=set(stopwords.words('english'))*\n",
        "\n",
        "* **The first line, nltk.download('stopwords'), downloads the list of stopwords from the Natural Language Toolkit (NLTK). Stopwords are words that are commonly used in a language, but that do not add much meaning to a sentence. For example, the words \"the\", \"a\", and \"and\" are all stopwords in English.**\n",
        "\n",
        "* **The second line, stemmer = nltk.SnowballStemmer(\"english\"), creates a Snowball stemmer object. A Snowball stemmer is a type of stemmer that uses a recursive algorithm to reduce a word to its stem or root. For example, the word \"playing\" would be stemmed to \"play\" by a Snowball stemmer.**\n",
        "\n",
        "* **The third line, stopword=set(stopwords.words('english')), creates a set of stopwords. The set() function is used to convert the list into a set data structure. The stopword set will contain all of the stopwords that were downloaded in the first line.**\n",
        "\n",
        "**These three lines of code are commonly used when processing text with NLTK. The stopwords are removed from the text to reduce the number of words that need to be processed, and the Snowball stemmer is used to reduce the words to their stems. This can make the text easier to analyze and can improve the performance of natural language processing algorithms.**"
      ],
      "metadata": {
        "id": "HsdU3anQEixJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the stopwords from the NLTK library.\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Create a SnowballStemmer object for the English language.\n",
        "stemmer = nltk.SnowballStemmer(\"english\")\n",
        "\n",
        "# Create a set of stopwords from the NLTK library.\n",
        "stopword=set(stopwords.words('english'))\n"
      ],
      "metadata": {
        "id": "xSmzRIlaCzoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(text):\n",
        "    text = str(text).lower()\n",
        "    # Convert the text to lowercase\n",
        "\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    # Remove all square brackets and their contents\n",
        "\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "    # Remove all URLs\n",
        "\n",
        "    text = re.sub('<.*?>+', '', text)\n",
        "    # Remove all HTML tags\n",
        "\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    # Remove all punctuation\n",
        "\n",
        "    text = re.sub('\\n', '', text)\n",
        "    # Remove all newline characters\n",
        "\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    # Remove all words that contain numbers\n",
        "\n",
        "    text = [word for word in text.split(' ') if word not in stopword]\n",
        "    # Remove all stopwords from the text\n",
        "\n",
        "    text = \" \".join(text)\n",
        "    # Join the words back together with spaces\n",
        "\n",
        "    text = [stemmer.stem(word) for word in text.split(' ')]\n",
        "    # Stem all of the words in the text\n",
        "\n",
        "    text = \" \".join(text)\n",
        "    # Join the stemmed words back together with spaces\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def analyze_sentiment(tweet):\n",
        "    \"\"\"\n",
        "    Analyze the sentiment of a tweet.\n",
        "\n",
        "    Args:\n",
        "        tweet: A string containing the tweet.\n",
        "\n",
        "    Returns:\n",
        "        An integer representing the sentiment of the tweet:\n",
        "        - 1 for positive sentiment\n",
        "        - 0 for neutral sentiment\n",
        "        - -1 for negative sentiment\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a TextBlob object from the tweet.\n",
        "    analysis = TextBlob(clean(tweet))\n",
        "\n",
        "    # Get the sentiment polarity of the tweet.\n",
        "    polarity = analysis.sentiment.polarity\n",
        "\n",
        "    # Return the sentiment of the tweet.\n",
        "    if polarity > 0:\n",
        "        return 1\n",
        "    elif polarity == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return -1\n"
      ],
      "metadata": {
        "id": "eep6ndbYcHzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adding columns to dataframe**"
      ],
      "metadata": {
        "id": "j1qhLLxEmaHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This line adds a sentiment column to the data frame, using the analyze_sentiment() function to determine the sentiment of each tweet.\n",
        "data['Sentiment'] = data['tweet'].apply(lambda x: analyze_sentiment(x))\n",
        "\n",
        "# This line adds a source column to the data frame, setting the source to \"random_user\" for all tweets.\n",
        "data['Source'] = 'random_user'\n",
        "\n",
        "# This line adds a length column to the data frame, counting the number of characters in each tweet.\n",
        "data['Length'] = data['tweet'].apply(len)\n",
        "\n",
        "# This line adds a word_counts column to the data frame, counting the number of words in each tweet.\n",
        "data['Word_counts'] = data['tweet'].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "# This line adds a clean_tweet column to the data frame, using the clean() function to clean each tweet.\n",
        "data['Clean tweet'] = data['tweet'].apply(lambda x: clean(x))\n"
      ],
      "metadata": {
        "id": "FQl_WqufcoI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "hCzScVuBeRGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This line creates a new data frame called data2, which contains only the tweet, sentiment, source, length, and word_counts columns from the original data frame.\n",
        "data2 = data[['tweet', 'Sentiment', 'Source', 'Length', 'Word_counts']]\n",
        "\n",
        "# This line prints the first five rows of the data2 data frame.\n",
        "data2.head()"
      ],
      "metadata": {
        "id": "7hWZn87vddfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating dataframes containing neutral,positive and negative sentiments"
      ],
      "metadata": {
        "id": "d6o6a0BumkUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "neutral = data[data['Sentiment'] == 0]\n",
        "positive = data[data['Sentiment'] == 1]\n",
        "negative = data[data['Sentiment'] == -1]"
      ],
      "metadata": {
        "id": "aXPUxRy9feWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neutral"
      ],
      "metadata": {
        "id": "grIAzy-IfhGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive"
      ],
      "metadata": {
        "id": "vtGWQ2IkjKUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative"
      ],
      "metadata": {
        "id": "QCz3YzyojOga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualization**"
      ],
      "metadata": {
        "id": "dlopCs1Cm1no"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This code creates a bar chart showing the distribution of sentiment in the dataset of tweets.\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "# Create a list of x-axis labels.\n",
        "x = ['Neutral', 'Positive', 'Negative']\n",
        "\n",
        "# Create a list of y-axis values.\n",
        "y = [len(neutral), len(positive), len(negative)]\n",
        "\n",
        "# Create a bar chart object.\n",
        "fig = go.Figure(data=[go.Bar(x=x, y=y, hovertext=['61% of tweets', '28% of tweets', '11% of tweets'])])\n",
        "\n",
        "# Customize the aspect of the bar chart.\n",
        "fig.update_traces(marker_line_color='midnightblue', marker_line_width=1.)\n",
        "\n",
        "# Set the title of the bar chart.\n",
        "fig.update_layout(title_text='Distribution of sentiment')\n",
        "\n",
        "# Display the bar chart.\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "xj_xoq2QfoaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code creates a pie chart showing the sentiment polarity of the invasion tweets dataset.\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "\n",
        "# Get the sentiment counts\n",
        "sizes = [count for count in data['Sentiment'].value_counts()]\n",
        "labels = list(data['Sentiment'].value_counts().index)\n",
        "\n",
        "# Set the pie chart properties\n",
        "explode = (0.1, 0, 0)\n",
        "ax.pie(x=sizes, labels=labels, autopct='%1.1f%%', explode=explode, textprops={'fontsize': 14})\n",
        "ax.set_title('Sentiment Polarity on invasion Tweets Data \\n (total = {})'.format(len(data)), fontsize=16, pad=20)\n",
        "\n",
        "# Show the pie chart\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tpdHxePNfy5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code prints out three examples of tweets, one each for neutral, positive, and negative sentiment.\n",
        "\n",
        "# Neutral tweet\n",
        "print(\"Neutral tweet example  :\", neutral['tweet'].values[15])\n",
        "# Comment: This is an example of a neutral tweet. It does not express any strong positive or negative sentiment.\n",
        "\n",
        "# Positive tweet\n",
        "print(\"Positive Tweet example :\", positive['tweet'].values[37])\n",
        "# Comment: This is an example of a positive tweet. It expresses happiness, excitement, or some other positive emotion.\n",
        "\n",
        "# Negative tweet\n",
        "print(\"Negative Tweet example :\", negative['tweet'].values[1])\n",
        "# Comment: This is an example of a negative tweet. It expresses sadness, anger, or some other negative emotion.\n"
      ],
      "metadata": {
        "id": "SHsjL6gof8g9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code creates a histogram showing the distribution of tweet lengths.\n",
        "x = data.Length.values\n",
        "\n",
        "fig = go.Figure(data=[go.Histogram(x=x,\n",
        "                                   marker_line_width=1, \n",
        "                                   marker_line_color=\"midnightblue\", \n",
        "                                   xbins_size = 5)])\n",
        "\n",
        "fig.update_layout(title_text='Distribution of tweet lengths')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "tdkrxrEkgE9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code creates histograms showing the distribution of tweet lengths for neutral, positive, and negative tweets.\n",
        "\n",
        "x1 = neutral.Length.values\n",
        "x2 = positive.Length.values\n",
        "x3 = negative.Length.values\n",
        "\n",
        "fig1 = go.Figure(data=[go.Histogram(x=x1,\n",
        "                                   marker_line_width=1, \n",
        "                                   marker_line_color=\"midnightblue\", \n",
        "                                   xbins_size = 5,\n",
        "                                   opacity = 1)])\n",
        "\n",
        "fig1.update_layout(title_text='Distribution of neutral tweet lengths')\n",
        "fig1.show()\n",
        "\n",
        "fig2 = go.Figure(data=[go.Histogram(x=x2,\n",
        "                                   marker_line_width=1, \n",
        "                                   marker_color='rgb(50,202,50)', \n",
        "                                   marker_line_color=\"midnightblue\", \n",
        "                                   xbins_size = 5,\n",
        "                                   opacity = 1)])\n",
        "\n",
        "fig2.update_layout(title_text='Distribution of positive tweet lengths')\n",
        "fig2.show()\n",
        "\n",
        "fig3 = go.Figure(data=[go.Histogram(x=x3,\n",
        "                                   marker_line_width=1, \n",
        "                                   marker_color='crimson', \n",
        "                                   marker_line_color=\"midnightblue\", \n",
        "                                   xbins_size = 5,\n",
        "                                   opacity = 1)])\n",
        "\n",
        "fig3.update_layout(title_text='Distribution of negative tweet lengths')\n",
        "fig3.show()"
      ],
      "metadata": {
        "id": "KR1FvARegUkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code creates a box plot showing the distribution of tweet lengths for neutral, positive, and negative tweets.\n",
        "\n",
        "y1 = neutral.Length.values\n",
        "y2 = positive.Length.values\n",
        "y3 = negative.Length.values\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Box(y=y1, \n",
        "                     name=\"Neutral\", \n",
        "                     marker_line_width=1, \n",
        "                     marker_line_color=\"midnightblue\"))\n",
        "\n",
        "fig.add_trace(go.Box(y=y2, \n",
        "                     name=\"Positive\", \n",
        "                     marker_line_width=1, \n",
        "                     marker_color = 'rgb(50,202,50)'))\n",
        "\n",
        "fig.add_trace(go.Box(y=y3, \n",
        "                     name=\"Negative\", \n",
        "                     marker_line_width=1, \n",
        "                     marker_color = 'crimson'))\n",
        "\n",
        "fig.update_layout(title_text=\"Box Plot tweet lengths\")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "So3650wAgbx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neutral"
      ],
      "metadata": {
        "id": "6nOW69wOhAdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Clean tweet']"
      ],
      "metadata": {
        "id": "vJyY-yYIKxs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code creates a new data frame called `tokenized_tweet`, which contains the lists of tweets in the `Clean tweet` column.\n",
        "tokenized_tweet = data['Clean tweet'].apply(lambda x: x.split())\n",
        "\n",
        "tokenized_tweet.head()"
      ],
      "metadata": {
        "id": "QiDNTyvBjlnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **PorterStemmer is a class in the Natural Language Toolkit (NLTK) that implements the Porter stemming algorithm. Stemming is a process of reducing a word to its root form. For example, the words \"running\", \"ran\", and \"runner\" would all be stemmed to their root word.**"
      ],
      "metadata": {
        "id": "obvJ_FpoOFp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining simple stem_words function to reduce words to its root form.\n",
        "# This code imports the PorterStemmer class from the nltk.stem.porter module.\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "def stem_words(words):\n",
        "  stemmer = PorterStemmer()\n",
        "  stemmed_words = []\n",
        "  for word in words:\n",
        "    stemmed_word = stemmer.stem(word)\n",
        "    stemmed_words.append(stemmed_word)\n",
        "  return stemmed_words\n",
        "\n",
        "words = [\"running\", \"ran\", \"runner\",\"going\"]\n",
        "stemmed_words = stem_words(words)\n",
        "print(stemmed_words)"
      ],
      "metadata": {
        "id": "DLDnEgIiNzFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code imports the PorterStemmer class from the nltk.stem.porter module.\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "# This creates a new instance of the PorterStemmer class.\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# This code applies the `lambda` function to each row in the `tokenized_tweet` data frame. The `lambda` function stems each token in the tweet.\n",
        "tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x])\n",
        "\n",
        "tokenized_tweet.head()"
      ],
      "metadata": {
        "id": "_fXIo_Omjo9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Word cloud of tweets**"
      ],
      "metadata": {
        "id": "c3QLiWmbRcJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This code creates a word cloud from lists of tweets.\n",
        "\n",
        "all_words = ' '.join([text for text in data['Clean tweet']])\n",
        "from wordcloud import WordCloud\n",
        "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
        "\n",
        "# This line creates a figure with a width of 10 inches and a height of 7 inches.\n",
        "plt.figure(figsize=(10, 7))\n",
        "\n",
        "# This line displays the word cloud.\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "\n",
        "# This line hides the axes.\n",
        "plt.axis('off')\n",
        "\n",
        "# This line shows the word cloud.\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bHTsTAnoj8-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Most Frequently Used Words**\n",
        "\n",
        "**Now let’s have a look at the wordcloud of the tweets, which will show the most frequently used words in the tweets by people sharing their feelings and updates about the Ukraine and Russia war:**"
      ],
      "metadata": {
        "id": "QqEPlUOpKOlc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \" \".join(i for i in data.tweet)\n",
        "stopwords = set(STOPWORDS)\n",
        "\n",
        "# This line creates a word cloud from the text, with stopwords removed and a black background.\n",
        "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110, stopwords=stopwords, background_color=\"black\").generate(text)\n",
        "plt.figure( figsize=(10,7))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lheBolat6XMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Positive Sentiments**\n",
        "\n",
        "**Now let’s have a look at the most frequent words used by people with positive sentiments**"
      ],
      "metadata": {
        "id": "gB_GyRJ9OS3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "positive_words = ' '.join([text for text in data['Clean tweet'][data['Sentiment'] == 1]])\n",
        "\n",
        "# This line creates a word cloud from the positive tweets\n",
        "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(positive_words)\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4SA0WIY6kA_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Negative Sentiments**\n",
        "\n",
        "**Now let’s have a look at the most frequent words used by people with negative sentiments**"
      ],
      "metadata": {
        "id": "Dp_wS4jYO_7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "negative_words = ' '.join([text for text in data['Clean tweet'][data['Sentiment'] == -1]])\n",
        "\n",
        "# This line creates a word cloud from the negative tweets\n",
        "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(negative_words)\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "E5KPlhK3kEVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Neutral Sentiments**\n",
        "\n",
        "**Now let’s have a look at the most frequent words used by people with neutral sentiments**"
      ],
      "metadata": {
        "id": "CymlvTKxP-FS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "neutral_words =' '.join([text for text in data['Clean tweet'][data['Sentiment'] == 0]])\n",
        "\n",
        "# This line creates a word cloud from the neutral tweets\n",
        "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(neutral_words)\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JcGjXb6AkIGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Selecting top 10 most frequent hashtags**"
      ],
      "metadata": {
        "id": "dG00rv1YB4Ey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HT = hashtag_extract(data['tweet'])\n",
        "HT = sum(HT,[])"
      ],
      "metadata": {
        "id": "bIg40JqYkPpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = nltk.FreqDist(HT)\n",
        "d = pd.DataFrame({'Hashtag': list(a.keys()),\n",
        "                  'Count': list(a.values())})\n",
        "# selecting top 10 most frequent hashtags     \n",
        "d = d.nlargest(columns=\"Count\", n = 10) \n",
        "plt.figure(figsize=(16,5))\n",
        "ax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\n",
        "ax.set(ylabel = 'Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "H4XzzuPVkTpu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}